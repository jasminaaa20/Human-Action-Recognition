{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import time \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram Equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_equalization(resized_frame):\n",
    "    # Send a resized frame to this function, and it will return the histogram equalized frame.\n",
    "    clahe = cv2.createCLAHE(clipLimit=10, tileGridSize=(8, 8))\n",
    "    img_gray = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2GRAY)\n",
    "    alpha = 2.2  # 1.0 - 3.0\n",
    "    beta = 50  # 0 - 100\n",
    "    \n",
    "    new_image = cv2.convertScaleAbs(img_gray, alpha=alpha, beta=beta)\n",
    "    gamma = 1.2\n",
    "    look_up_table = np.empty((1, 256), np.uint8)\n",
    "    for i in range(256):\n",
    "        look_up_table[0, i] = np.clip(pow(i / 255.0, gamma) * 255.0, 0, 255)\n",
    "    \n",
    "    res = cv2.LUT(new_image, look_up_table)\n",
    "    final_img = clahe.apply(res)\n",
    "    \n",
    "    return final_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MediaPipe implemetation for a singole person. 32 points per frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works for Single person. Have to extend this to multi people . lmList contains keyponts foe all 32 points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FILE = \"F:\\CS3501-Data_Science_and_Engineering_Project\\Project Files\\Human-Action-Recognition-in-the-dark\\datasets\\clips_v1.5\\Jump\\Jump_1_7.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpPose = mp.solutions.pose\n",
    "pose = mpPose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence = 0.5, model_complexity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mf:\\CS3501-Data_Science_and_Engineering_Project\\Project Files\\Human-Action-Recognition-in-the-dark\\notebooks\\poseDitection.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/CS3501-Data_Science_and_Engineering_Project/Project%20Files/Human-Action-Recognition-in-the-dark/notebooks/poseDitection.ipynb#X10sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mif\u001b[39;00m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m1\u001b[39m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mq\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/CS3501-Data_Science_and_Engineering_Project/Project%20Files/Human-Action-Recognition-in-the-dark/notebooks/poseDitection.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/CS3501-Data_Science_and_Engineering_Project/Project%20Files/Human-Action-Recognition-in-the-dark/notebooks/poseDitection.ipynb#X10sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(cv2\u001b[39m.\u001b[39;49mcvtColor(img, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2RGB))  \n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/CS3501-Data_Science_and_Engineering_Project/Project%20Files/Human-Action-Recognition-in-the-dark/notebooks/poseDitection.ipynb#X10sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m cap\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/CS3501-Data_Science_and_Engineering_Project/Project%20Files/Human-Action-Recognition-in-the-dark/notebooks/poseDitection.ipynb#X10sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "mpDraw = mp.solutions.drawing_utils\n",
    "cap = cv2.VideoCapture(VIDEO_FILE)\n",
    "ptime = 0\n",
    "points = []\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    \n",
    "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    imgRGB.flags.writeable = False\n",
    "    results = pose.process(imgRGB)\n",
    "    imgRGB.flags.writeable = True\n",
    "    \n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        mpDraw.draw_landmarks(img, results.pose_landmarks, mpPose.POSE_CONNECTIONS)\n",
    "        lmList = [] #list of points for each frame\n",
    "        for id, lm in enumerate( results.pose_landmarks.landmark):\n",
    "            h, w, c = img.shape\n",
    "            cx, cy = int(lm.x*w), int(lm.y*h)\n",
    "            lmList.append([id, cx, cy])\n",
    "        if len(lmList) != 0:\n",
    "            points.append(lmList)       \n",
    "            cv2.circle(img, (lmList[14][1], lmList[14][2]), 9, (255, 0, 0), cv2.FILLED)\n",
    "        \n",
    "    cTime = time.time()\n",
    "    fps = 1/(cTime-ptime)\n",
    "    ptime = cTime\n",
    "    \n",
    "    font_size = 3\n",
    "    color = (255, 0, 0)\n",
    "    cv2.putText(img, str(int(fps)), (10, 70), cv2.FONT_HERSHEY_PLAIN, font_size, color, 3)\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multipose Movenet implementation for up to 6 people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the input_keypoints variable as the input to a model we are buidling. It contains 6 arrays which corresponds to maximum of six people this model can predict. Then it has 51 data points x, y and score * 17. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional if you are using a gpu\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_model = hub.load(\"https://tfhub.dev/google/movenet/multipose/lightning/1\")\n",
    "movenet = hub_model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"../vid.mp4\")\n",
    "ptime = 0\n",
    "\n",
    "while True:\n",
    "    sucess, frame = cap.read()\n",
    "    if not sucess:\n",
    "        print(\"video is over\")\n",
    "        break\n",
    "    \n",
    "    # resize the frame to make prediction faster. Longerside should be > 256 and multiple of 32\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 320, 512)\n",
    "    input_img = tf.cast(img, dtype=tf.int32) \n",
    "    \n",
    "    # make prediction\n",
    "    \n",
    "    results = movenet(input_img)\n",
    "    #each result has 56 points that is x,y,score * 17 + 5 bounding box coordinates\n",
    "    keypoints = results[\"output_0\"].numpy()[:,:,:51].reshape(6,17,3) # 6 people, 17 key points, 3 for each key point\n",
    "    \n",
    "    #make sure it is confident enough to run through the model\n",
    "    x, y, s = frame.shape\n",
    "    input_keypoints = np.squeeze(np.multiply(keypoints, [y, x, 1]))\n",
    "    \n",
    "    #Render\n",
    "    loop_through_people(frame, keypoints, EDGES, 0.3)\n",
    "    \n",
    "    cv2.imshow(\"Image\", frame)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_people(frame, keypoints_with_score, edges, confidence_threshold=0.4):\n",
    "    for person in keypoints_with_score:\n",
    "        draw_connections(frame, person, edges, confidence_threshold)\n",
    "        draw_person(frame, person, confidence_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_person(frame, person, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(person, [y, x, 1]))\n",
    "    for person in shaped:\n",
    "        #creating an array for preditions\n",
    "        ky, kx, kp_conf = person\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 4, (0, 255, 0), -1)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
