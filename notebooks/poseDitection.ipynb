{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import time \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MediaPipe implemetation for a singole person. 32 points per frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works for Single person. Have to extend this to multi people . lmList contains keyponts foe all 32 points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FILE = \"F:\\CS3501-Data_Science_and_Engineering_Project\\Project Files\\Human-Action-Recognition-in-the-dark\\datasets\\clips_v1.5\\Jump\\Jump_1_7.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpPose = mp.solutions.pose\n",
    "pose = mpPose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence = 0.5, model_complexity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mf:\\CS3501-Data_Science_and_Engineering_Project\\Project Files\\Human-Action-Recognition-in-the-dark\\notebooks\\poseDitection.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/CS3501-Data_Science_and_Engineering_Project/Project%20Files/Human-Action-Recognition-in-the-dark/notebooks/poseDitection.ipynb#W5sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mif\u001b[39;00m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m1\u001b[39m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mq\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/CS3501-Data_Science_and_Engineering_Project/Project%20Files/Human-Action-Recognition-in-the-dark/notebooks/poseDitection.ipynb#W5sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/CS3501-Data_Science_and_Engineering_Project/Project%20Files/Human-Action-Recognition-in-the-dark/notebooks/poseDitection.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(cv2\u001b[39m.\u001b[39;49mcvtColor(img, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2RGB))  \n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "mpDraw = mp.solutions.drawing_utils\n",
    "cap = cv2.VideoCapture(VIDEO_FILE)\n",
    "ptime = 0\n",
    "points = []\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    \n",
    "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    imgRGB.flags.writeable = False\n",
    "    results = pose.process(imgRGB)\n",
    "    imgRGB.flags.writeable = True\n",
    "    \n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        mpDraw.draw_landmarks(img, results.pose_landmarks, mpPose.POSE_CONNECTIONS)\n",
    "        lmList = [] #list of points for each frame\n",
    "        for id, lm in enumerate( results.pose_landmarks.landmark):\n",
    "            h, w, c = img.shape\n",
    "            cx, cy = int(lm.x*w), int(lm.y*h)\n",
    "            lmList.append([id, cx, cy])\n",
    "        if len(lmList) != 0:\n",
    "            points.append(lmList)       \n",
    "            cv2.circle(img, (lmList[14][1], lmList[14][2]), 9, (255, 0, 0), cv2.FILLED)\n",
    "        \n",
    "    cTime = time.time()\n",
    "    fps = 1/(cTime-ptime)\n",
    "    ptime = cTime\n",
    "    \n",
    "    font_size = 3\n",
    "    color = (255, 0, 0)\n",
    "    cv2.putText(img, str(int(fps)), (10, 70), cv2.FONT_HERSHEY_PLAIN, font_size, color, 3)\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multipose Movenet implementation for up to 6 people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the input_keypoints variable as the input to a model we are buidling. It contains 6 arrays which corresponds to maximum of six people this model can predict. Then it has 51 data points x, y and score * 17. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional if you are using a gpu\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 11001] getaddrinfo failed>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Python311\\Lib\\urllib\\request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1348\u001b[0m     h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[0;32m   1349\u001b[0m               encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m   1350\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\http\\client.py:1282\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1282\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\http\\client.py:1328\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1327\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1328\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\http\\client.py:1277\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1277\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\http\\client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1037\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[0;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1040\u001b[0m \n\u001b[0;32m   1041\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\http\\client.py:975\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[1;32m--> 975\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m    976\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\http\\client.py:1447\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mConnect to a host on a given (SSL) port.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1447\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tunnel_host:\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\http\\client.py:941\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    940\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m\"\u001b[39m\u001b[39mhttp.client.connect\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mport)\n\u001b[1;32m--> 941\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection(\n\u001b[0;32m    942\u001b[0m     (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhost,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address)\n\u001b[0;32m    943\u001b[0m \u001b[39m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\socket.py:826\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[0;32m    825\u001b[0m exceptions \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 826\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m getaddrinfo(host, port, \u001b[39m0\u001b[39;49m, SOCK_STREAM):\n\u001b[0;32m    827\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\socket.py:961\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    960\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 961\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[0;32m    962\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mf:\\CS3501-Data_Science_and_Engineering_Project\\Project Files\\Human-Action-Recognition-in-the-dark\\notebooks\\poseDitection.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/CS3501-Data_Science_and_Engineering_Project/Project%20Files/Human-Action-Recognition-in-the-dark/notebooks/poseDitection.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m hub_model \u001b[39m=\u001b[39m hub\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mhttps://tfhub.dev/google/movenet/multipose/lightning/1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/CS3501-Data_Science_and_Engineering_Project/Project%20Files/Human-Action-Recognition-in-the-dark/notebooks/poseDitection.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m movenet \u001b[39m=\u001b[39m hub_model\u001b[39m.\u001b[39msignatures[\u001b[39m'\u001b[39m\u001b[39mserving_default\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mf:\\CS3501-Data_Science_and_Engineering_Project\\Project Files\\Human-Action-Recognition-in-the-dark\\.venv\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:93\u001b[0m, in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m     92\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected a string, got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m handle)\n\u001b[1;32m---> 93\u001b[0m module_path \u001b[39m=\u001b[39m resolve(handle)\n\u001b[0;32m     94\u001b[0m is_hub_module_v1 \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(\n\u001b[0;32m     95\u001b[0m     native_module\u001b[39m.\u001b[39mget_module_proto_path(module_path))\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m tags \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m is_hub_module_v1:\n",
      "File \u001b[1;32mf:\\CS3501-Data_Science_and_Engineering_Project\\Project Files\\Human-Action-Recognition-in-the-dark\\.venv\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:48\u001b[0m, in \u001b[0;36mresolve\u001b[1;34m(handle)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresolve\u001b[39m(handle):\n\u001b[0;32m     25\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Resolves a module handle into a path.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[39m  This function works both for plain TF2 SavedModels and the legacy TF1 Hub\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39m    A string representing the Module path.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m   \u001b[39mreturn\u001b[39;00m registry\u001b[39m.\u001b[39;49mresolver(handle)\n",
      "File \u001b[1;32mf:\\CS3501-Data_Science_and_Engineering_Project\\Project Files\\Human-Action-Recognition-in-the-dark\\.venv\\Lib\\site-packages\\tensorflow_hub\\registry.py:49\u001b[0m, in \u001b[0;36mMultiImplRegister.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m impl \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impls):\n\u001b[0;32m     48\u001b[0m   \u001b[39mif\u001b[39;00m impl\u001b[39m.\u001b[39mis_supported(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     50\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     fails\u001b[39m.\u001b[39mappend(\u001b[39mtype\u001b[39m(impl)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n",
      "File \u001b[1;32mf:\\CS3501-Data_Science_and_Engineering_Project\\Project Files\\Human-Action-Recognition-in-the-dark\\.venv\\Lib\\site-packages\\tensorflow_hub\\compressed_module_resolver.py:81\u001b[0m, in \u001b[0;36mHttpCompressedFileResolver.__call__\u001b[1;34m(self, handle)\u001b[0m\n\u001b[0;32m     77\u001b[0m   response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_urlopen(request)\n\u001b[0;32m     78\u001b[0m   \u001b[39mreturn\u001b[39;00m resolver\u001b[39m.\u001b[39mDownloadManager(handle)\u001b[39m.\u001b[39mdownload_and_uncompress(\n\u001b[0;32m     79\u001b[0m       response, tmp_dir)\n\u001b[1;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m resolver\u001b[39m.\u001b[39;49matomic_download(handle, download, module_dir,\n\u001b[0;32m     82\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lock_file_timeout_sec())\n",
      "File \u001b[1;32mf:\\CS3501-Data_Science_and_Engineering_Project\\Project Files\\Human-Action-Recognition-in-the-dark\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:421\u001b[0m, in \u001b[0;36matomic_download\u001b[1;34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001b[0m\n\u001b[0;32m    419\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mDownloading TF-Hub Module \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, handle)\n\u001b[0;32m    420\u001b[0m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mMakeDirs(tmp_dir)\n\u001b[1;32m--> 421\u001b[0m download_fn(handle, tmp_dir)\n\u001b[0;32m    422\u001b[0m \u001b[39m# Write module descriptor to capture information about which module was\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39m# downloaded by whom and when. The file stored at the same level as a\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[39m# directory in order to keep the content of the 'model_dir' exactly as it\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39m# module caching protocol and no code in the TF-Hub library reads its\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[39m# content.\u001b[39;00m\n\u001b[0;32m    431\u001b[0m _write_module_descriptor_file(handle, module_dir)\n",
      "File \u001b[1;32mf:\\CS3501-Data_Science_and_Engineering_Project\\Project Files\\Human-Action-Recognition-in-the-dark\\.venv\\Lib\\site-packages\\tensorflow_hub\\compressed_module_resolver.py:77\u001b[0m, in \u001b[0;36mHttpCompressedFileResolver.__call__.<locals>.download\u001b[1;34m(handle, tmp_dir)\u001b[0m\n\u001b[0;32m     71\u001b[0m   \u001b[39mreturn\u001b[39;00m resolver\u001b[39m.\u001b[39mDownloadManager(handle)\u001b[39m.\u001b[39mdownload_and_uncompress(\n\u001b[0;32m     72\u001b[0m       response, tmp_dir\n\u001b[0;32m     73\u001b[0m   )\n\u001b[0;32m     75\u001b[0m request \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mRequest(\n\u001b[0;32m     76\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_compressed_format_query(handle))\n\u001b[1;32m---> 77\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_urlopen(request)\n\u001b[0;32m     78\u001b[0m \u001b[39mreturn\u001b[39;00m resolver\u001b[39m.\u001b[39mDownloadManager(handle)\u001b[39m.\u001b[39mdownload_and_uncompress(\n\u001b[0;32m     79\u001b[0m     response, tmp_dir)\n",
      "File \u001b[1;32mf:\\CS3501-Data_Science_and_Engineering_Project\\Project Files\\Human-Action-Recognition-in-the-dark\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:528\u001b[0m, in \u001b[0;36mHttpResolverBase._call_urlopen\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    526\u001b[0m   \u001b[39mreturn\u001b[39;00m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlopen(request)\n\u001b[0;32m    527\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 528\u001b[0m   \u001b[39mreturn\u001b[39;00m urllib\u001b[39m.\u001b[39;49mrequest\u001b[39m.\u001b[39;49murlopen(request, context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\urllib\\request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    516\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[0;32m    518\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[1;32m--> 519\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[0;32m    521\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[0;32m    522\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\urllib\\request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m    535\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[1;32m--> 536\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[0;32m    537\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[0;32m    539\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    497\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\urllib\\request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[1;32m-> 1391\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPSConnection, req,\n\u001b[0;32m   1392\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context, check_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_hostname)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\urllib\\request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m         h\u001b[39m.\u001b[39mrequest(req\u001b[39m.\u001b[39mget_method(), req\u001b[39m.\u001b[39mselector, req\u001b[39m.\u001b[39mdata, headers,\n\u001b[0;32m   1349\u001b[0m                   encode_chunked\u001b[39m=\u001b[39mreq\u001b[39m.\u001b[39mhas_header(\u001b[39m'\u001b[39m\u001b[39mTransfer-encoding\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m   1350\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[1;32m-> 1351\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n\u001b[0;32m   1352\u001b[0m     r \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m   1353\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [Errno 11001] getaddrinfo failed>"
     ]
    }
   ],
   "source": [
    "hub_model = hub.load(\"https://tfhub.dev/google/movenet/multipose/lightning/1\")\n",
    "movenet = hub_model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"../vid.mp4\")\n",
    "ptime = 0\n",
    "\n",
    "while True:\n",
    "    sucess, frame = cap.read()\n",
    "    if not sucess:\n",
    "        print(\"video is over\")\n",
    "        break\n",
    "    \n",
    "    # resize the frame to make prediction faster. Longerside should be > 256 and multiple of 32\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 320, 512)\n",
    "    input_img = tf.cast(img, dtype=tf.int32) \n",
    "    \n",
    "    # make prediction\n",
    "    \n",
    "    results = movenet(input_img)\n",
    "    #each result has 56 points that is x,y,score * 17 + 5 bounding box coordinates\n",
    "    keypoints = results[\"output_0\"].numpy()[:,:,:51].reshape(6,17,3) # 6 people, 17 key points, 3 for each key point\n",
    "    \n",
    "    #make sure it is confident enough to run through the model\n",
    "    x, y, s = frame.shape\n",
    "    input_keypoints = np.squeeze(np.multiply(keypoints, [y, x, 1]))\n",
    "    \n",
    "    #Render\n",
    "    loop_through_people(frame, keypoints, EDGES, 0.3)\n",
    "    \n",
    "    cv2.imshow(\"Image\", frame)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_people(frame, keypoints_with_score, edges, confidence_threshold=0.4):\n",
    "    for person in keypoints_with_score:\n",
    "        draw_connections(frame, person, edges, confidence_threshold)\n",
    "        draw_person(frame, person, confidence_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_person(frame, person, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(person, [y, x, 1]))\n",
    "    for person in shaped:\n",
    "        #creating an array for preditions\n",
    "        ky, kx, kp_conf = person\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 4, (0, 255, 0), -1)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
